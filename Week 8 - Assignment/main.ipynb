{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bdccc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import SVHN dataset\n",
    "from torchvision.datasets import SVHN\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16649665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6161003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# Data preparation with normalization specific to SVHN\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # Resize images to 224x224 for models like ResNet\n",
    "    transforms.ToTensor(), # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_set = datasets.SVHN(root='./data', split='train', download=True, transform=transform_train)\n",
    "test_set = datasets.SVHN(root='./data', split='test', download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = train_set.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30af1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet32(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = 16\n",
    "\n",
    "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.layer1 = self._make_layer(16, 5, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 5, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 5, stride=2)\n",
    "\n",
    "        # adaptive pooling ensures correct size regardless of input\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride):\n",
    "        strides = [stride] + [1]*(blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))  # fixed pooling\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73a99b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 5,227,961\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-defined model (ResNet32) without pre-trained weights\n",
    "model = ResNet32(num_classes=len(classes)).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total trainable parameters: {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3bc57bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Loss function for multi-class classification\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001) # Adam optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b18cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   6%|▋         | 145/2290 [00:42<10:31,  3.40batch/s, loss=4.14]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m loss.backward()\n\u001b[32m     20\u001b[39m optimiser.step()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     24\u001b[39m     progress_bar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: running_loss / \u001b[32m100\u001b[39m})\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "patience = 8\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}', unit='batch')\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(progress_bar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            progress_bar.set_postfix({'loss': running_loss / 100})\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = correct / total\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"  ✓ New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # ---- PERIODIC CHECKPOINT ----\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        print(f\"  ✓ Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "    # ---- EARLY STOPPING ----\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: val_acc={val_acc:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state dict\n",
    "save_path = \"svhn_cnn.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd55362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the model architecture first\n",
    "model = ResNet32(num_classes=len(classes)).to(device)\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(\"svhn_cnn.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy().transpose(1, 2, 0)\n",
    "    plt.imshow(npimg)\n",
    "    plt.axis('off')\n",
    "\n",
    "def show_predictions(model, test_loader, device, max_correct=10, max_wrong=10):\n",
    "    model.eval()\n",
    "    correct_imgs = []\n",
    "    wrong_imgs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels[labels == 10] = 0\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for img, pred, true in zip(images, predicted, labels):\n",
    "                if pred == true and len(correct_imgs) < max_correct:\n",
    "                    correct_imgs.append((img.cpu(), int(pred), int(true)))\n",
    "                elif pred != true and len(wrong_imgs) < max_wrong:\n",
    "                    wrong_imgs.append((img.cpu(), int(pred), int(true)))\n",
    "\n",
    "                if len(correct_imgs) == max_correct and len(wrong_imgs) == max_wrong:\n",
    "                    break\n",
    "            if len(correct_imgs) == max_correct and len(wrong_imgs) == max_wrong:\n",
    "                break\n",
    "\n",
    "    # Display them\n",
    "    print(\"\\nCorrect Predictions:\\n\")\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i, (img, pred, true) in enumerate(correct_imgs):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        imshow(img)\n",
    "        plt.title(f\"Pred: {pred}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nIncorrect Predictions:\\n\")\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i, (img, pred, true) in enumerate(wrong_imgs):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        imshow(img)\n",
    "        plt.title(f\"Pred: {pred}, True: {true}\")\n",
    "    plt.show()\n",
    "\n",
    "# Run it:\n",
    "show_predictions(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
